# Source

[Securing AI Model Weights: Preventing Theft and Misuse of Frontier Models](https://github.com/olga-sorokoletova/Security-Mitigations/blob/main/Securing%20AI%20Model%20Weights/paper.pdf)

# Scope

**Objective:** Securing model assets.[^1]\
**Type of asset:** model weights.[^2]\
**Risks:** theft, copying, and mimicry.

[^1]: **Other AI Security concerns:** 1. Protecting AI system confidentiality, integrity, and availability; 2. Guarding against misuse of legitimate API; 3. Harm mitigations after the exfiltration; 4. Development of a robust R&D agenda to expand the AI security toolkit; 5. Researching the roles of different actors in securing AI systems.
[^2]: **Other assets subject to protection:** 1. Training data; 2. Algorithmic insights; 3. Model architecture design; 4. Operational infrastructure; 5. Source code.

# Attack Vectors
## Taxonomy of Attack Vectors
Taxonomy consists of 38 [atack vectors](https://github.com/olga-sorokoletova/Security-Mitigations/blob/main/Securing%20AI%20Model%20Weights/attack%20vectors.md) relevant for the objective of securing AI model weights and divided into 9 categories, including the *AI-Specific Attack Vectors* category.

## AI-Specific Attack Vectors
**AI-Specific Attack Vectors** $-$ attacks that target AI infrastructure and are only relevant for AI systems:
- AIMED AT CODE EXECUTION
  - 1. Discovering Existing Vulnerabilities in the Machine Learning Stack
  - 2. Intentional Machine Learning Supply Chain Compromise  
- MANIPULATING MODEL OUTPUTS
  - 3. Prompt-Triggered Code Execution (Prompt Injection)
- MODEL DERIVATION
  - 4. Model Extraction
  - 5. Model Distillation
   



